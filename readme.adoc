= Falcon VM backing tests

We've observed very poor performance in Falcon VMs with heavy I/O workloads.
This is an effort to figure out what's going on. The poor performance not only
includes slow I/O but VMs seem to lock up entirely for periods of seconds to
minutes.

The poor performance has been observed in topologies where there are 4 VMs
running I/O heavy workloads.

== Topology under test

All of the tests here have the same topology.

- 4 virtual machines
- 100G virtual disks using `virtio-block`

When falcon sets up a guest zvol it looks like this

<details>
<pre>
$ zfs get all apool/falcon/topo/fback/a
NAME                       PROPERTY              VALUE                             SOURCE
apool/falcon/topo/fback/a  type                  volume                            -
apool/falcon/topo/fback/a  creation              Sat Mar 16 12:48 2024             -
apool/falcon/topo/fback/a  used                  56K                               -
apool/falcon/topo/fback/a  available             1.64T                             -
apool/falcon/topo/fback/a  referenced            4.23G                             -
apool/falcon/topo/fback/a  compressratio         1.00x                             -
apool/falcon/topo/fback/a  origin                apool/falcon/img/helios-2.0@base  -
apool/falcon/topo/fback/a  reservation           20G                               local
apool/falcon/topo/fback/a  volsize               20G                               local
apool/falcon/topo/fback/a  volblocksize          4K                                -
apool/falcon/topo/fback/a  checksum              on                                default
apool/falcon/topo/fback/a  compression           off                               default
apool/falcon/topo/fback/a  readonly              off                               default
apool/falcon/topo/fback/a  createtxg             48310                             -
apool/falcon/topo/fback/a  copies                1                                 default
apool/falcon/topo/fback/a  refreservation        none                              default
apool/falcon/topo/fback/a  guid                  15380078609976307608              -
apool/falcon/topo/fback/a  primarycache          all                               default
apool/falcon/topo/fback/a  secondarycache        all                               default
apool/falcon/topo/fback/a  usedbysnapshots       0B                                -
apool/falcon/topo/fback/a  usedbydataset         56K                               -
apool/falcon/topo/fback/a  usedbychildren        0B                                -
apool/falcon/topo/fback/a  usedbyrefreservation  0B                                -
apool/falcon/topo/fback/a  logbias               latency                           default
apool/falcon/topo/fback/a  dedup                 off                               default
apool/falcon/topo/fback/a  mlslabel              none                              default
apool/falcon/topo/fback/a  sync                  standard                          default
apool/falcon/topo/fback/a  refcompressratio      1.00x                             -
apool/falcon/topo/fback/a  written               56K                               -
apool/falcon/topo/fback/a  logicalused           28K                               -
apool/falcon/topo/fback/a  logicalreferenced     4.22G                             -
apool/falcon/topo/fback/a  snapshot_limit        none                              default
apool/falcon/topo/fback/a  snapshot_count        none                              default
apool/falcon/topo/fback/a  redundant_metadata    all                               default
apool/falcon/topo/fback/a  encryption            off                               default
apool/falcon/topo/fback/a  keylocation           none                              default
apool/falcon/topo/fback/a  keyformat             none                              default
apool/falcon/topo/fback/a  pbkdf2iters           0                                 default
</pre>
</details>

== Test 1: Zvol backing versus regular files

This test evaluates creating a backing block device by cloning a zvol snapshot
versus copying a zvol snapshot into a regular file.

When a cloned snapshot is used, the backing device is the corresponding
`/dev/zvol/(r)dsk`. There were only small differences between `rdsk` and `dsk`,
surprisingly with `dsk` coming out a bit better, so `dsk` was used for these
benchmarks. 

When a file is used, the backing snapshot is copied into the backing file with `dd`.

In this test we run the following `fio` tests within all 4 VMs simultaneously.

4kB blocks

----
fio \
  --name=random-write \
  --ioengine=posixaio \
  --rw=randwrite \
  --bs=4k \
  --numjobs=4 \
  --size=4g \
  --runtime=60 \
  --time_based \
  --iodepth=8 \
  --end_fsync=1
----

512B blocks

----
fio \
  --name=random-write \
  --ioengine=posixaio \
  --rw=randwrite \
  --bs=512 \
  --numjobs=4 \
  --size=4g \
  --runtime=60 \
  --time_based \
  --iodepth=8 \
  --end_fsync=1
----

The results are in `results/test1/<node-name>`. To get a sense for load on the
host we run `w` on the host right after the test is complete, that result is in
`results/test1/host`.

=== Result summary

==== Zvol

4k block bandwidth

[cols="1,1"]
|===
|Node | Bandwidth

|a
|4891 KiB/s

|b
|4922 KiB/s

|c
|4861 KiB/s

|d
| 4854 KiB/s
|===

host load average: 7.12, 3.37, 2.19

512 block bandwidth 

[cols="1,1"]
|===
|Node | Bandwidth

|a
|655 KiB/s

|b
|700 KiB/s

|c
|890 KiB/s

|d
|857 KiB/s
|===

host load average: 6.24, 2.47, 1.77

==== File

4k block bandwidth

[cols="1,1"]
|===
|Node | Bandwidth

|a
|24.7 MiB/s

|b
|24.8 MiB/s

|c
|25.0 MiB/s

|d
|25.3 MiB/s
|===

host load average: 13.07, 6.75, 4.05

512 block bandwidth 

[cols="1,1"]
|===
|Node | Bandwidth

|a
|3240 KiB/s

|b
|3235 KiB/s

|c
|3243 KiB/s

|d
|3241 KiB/s
|===

host load average: 11.30, 5.73, 3.27
